#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±åˆã‚«ãƒ¼ã‚»ãƒ³ã‚µãƒ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰
å–å¾—æ—¥æ™‚ã¨è»Šä¸¡URLã®è¨˜éŒ²æ©Ÿèƒ½ã‚’è¿½åŠ 
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
import re
import logging
from datetime import datetime
from urllib.parse import urljoin, urlparse, parse_qs
from pathlib import Path

class CarScraper:
    def __init__(self, output_dir=None):
        if output_dir is None:
            output_dir = Path("data") / "scraped"
        else:
            output_dir = Path(output_dir)

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.setup_logging()
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        })
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        self.scraping_start_time = datetime.now()
        
    def setup_logging(self):
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def sanitize_filename(self, name):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ãˆãªã„æ–‡å­—ã‚’ç½®æ›ãƒ»å‰Šé™¤"""
        name = name.replace('ãƒ»', '_').replace(' ', '_')
        return re.sub(r'[\\|/|:|*|?|"|<|>|\|]', '', name)
    
    def extract_car_name(self, soup, url):
        """è»Šç¨®åã‚’æŠ½å‡º"""
        selectors = [
            ('h2.title1', r'(.+?)ï¼ˆå…¨å›½ï¼‰ã®ä¸­å¤è»Š'),
            ('h1', r'(.+?)\s*ã®ä¸­å¤è»Š'),
            ('title', r'(.+?)\s*ã®ä¸­å¤è»Š')
        ]
        
        for selector, pattern in selectors:
            element = soup.select_one(selector)
            if element:
                match = re.search(pattern, element.get_text())
                if match:
                    full_name = match.group(1).strip()
                    return full_name.split()[-1].split('ãƒ»')[-1]
        
        self.logger.warning(f"è»Šç¨®åã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ: {url}")
        return "Unknown"
    
    def extract_vehicle_url(self, item):
        """è»Šä¸¡å€‹åˆ¥URLã‚’æŠ½å‡º"""
        try:
            # è»Šä¸¡è©³ç´°ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            link_selectors = [
                'h3.cassetteMain__title a',
                'a[href*="/usedcar/detail/"]',
                '.cassette__link',
                'a.js_detail_link'
            ]
            
            for selector in link_selectors:
                link_element = item.select_one(selector)
                if link_element and link_element.has_attr('href'):
                    href = link_element['href']
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if href.startswith('/'):
                        vehicle_url = f"https://www.carsensor.net{href}"
                    elif href.startswith('http'):
                        vehicle_url = href
                    else:
                        vehicle_url = f"https://www.carsensor.net/usedcar/detail/{href}"
                    
                    self.logger.debug(f"è»Šä¸¡URLæŠ½å‡ºæˆåŠŸ: {vehicle_url}")
                    return vehicle_url
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ¼ã‚¿å±æ€§ã‹ã‚‰è»Šä¸¡IDã‚’æŠ½å‡º
            data_attrs = ['data-detail-url', 'data-vehicle-id', 'data-car-id']
            for attr in data_attrs:
                if item.has_attr(attr):
                    vehicle_id = item[attr]
                    return f"https://www.carsensor.net/usedcar/detail/{vehicle_id}/index.html"
            
            self.logger.warning("è»Šä¸¡URLã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return ""
            
        except Exception as e:
            self.logger.warning(f"è»Šä¸¡URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def parse_car_item(self, item, car_name, base_url):
        """å€‹åˆ¥è»Šä¸¡ã‚¢ã‚¤ãƒ†ãƒ ã®è§£æï¼ˆURLæŠ½å‡ºæ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            # è»Šä¸¡å€‹åˆ¥URLã‚’æŠ½å‡º
            vehicle_url = self.extract_vehicle_url(item)
            
            # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚°ãƒ¬ãƒ¼ãƒ‰
            title_tag = item.find('h3', class_='cassetteMain__title')
            full_title = title_tag.get_text(strip=True) if title_tag else 'æƒ…å ±ãªã—'
            grade = full_title.replace(car_name, '', 1).strip()
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
            model_tag = item.find('p', class_='cassetteMain__tag')
            model_info = model_tag.get_text(strip=True) if model_tag else 'æƒ…å ±ãªã—'
            
            # ä¾¡æ ¼
            price_tag = item.find('div', class_='totalPrice')
            if price_tag and price_tag.find('p', class_='totalPrice__content'):
                price = price_tag.find('p', class_='totalPrice__content').get_text(strip=True)
            else:
                price = 'å¿œè«‡'
            
            # ä»•æ§˜æƒ…å ±
            spec_data = {
                'å¹´å¼': 'æƒ…å ±ãªã—',
                'èµ°è¡Œè·é›¢': 'æƒ…å ±ãªã—', 
                'ä¿®å¾©æ­´': 'æƒ…å ±ãªã—',
                'ãƒŸãƒƒã‚·ãƒ§ãƒ³': 'æƒ…å ±ãªã—',
                'æ’æ°—é‡': 'æƒ…å ±ãªã—'
            }
            
            spec_items = item.select('dl.specList > div.specList__detailBox')
            for spec_item in spec_items:
                dt = spec_item.find('dt')
                dd = spec_item.find('dd')
                if dt and dd:
                    label = dt.get_text(strip=True)
                    value = dd.get_text(strip=True)
                    if label in spec_data:
                        spec_data[label] = value
            
            # ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—æ—¥æ™‚ã¨ã—ã¦è¨˜éŒ²
            current_time = datetime.now()
            
            return {
                'è»Šç¨®å': car_name,
                'ãƒ¢ãƒ‡ãƒ«': model_info,
                'ã‚°ãƒ¬ãƒ¼ãƒ‰': grade,
                'æ”¯æ‰•ç·é¡': price,
                'å¹´å¼': spec_data['å¹´å¼'],
                'èµ°è¡Œè·é›¢': spec_data['èµ°è¡Œè·é›¢'],
                'ä¿®å¾©æ­´': spec_data['ä¿®å¾©æ­´'],
                'ãƒŸãƒƒã‚·ãƒ§ãƒ³': spec_data['ãƒŸãƒƒã‚·ãƒ§ãƒ³'],
                'æ’æ°—é‡': spec_data['æ’æ°—é‡'],
                'å–å¾—æ—¥æ™‚': current_time.isoformat(),
                'å–å¾—æ—¥': current_time.strftime('%Y-%m-%d'),
                'å–å¾—æ™‚åˆ»': current_time.strftime('%H:%M:%S'),
                'ã‚½ãƒ¼ã‚¹URL': base_url,
                'è»Šä¸¡URL': vehicle_url
            }
            
        except Exception as e:
            self.logger.warning(f"è»Šä¸¡ã‚¢ã‚¤ãƒ†ãƒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def scrape_url(self, url, max_pages=10, max_items_per_page=30):
        """å˜ä¸€URLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆURLè¨˜éŒ²æ©Ÿèƒ½ä»˜ãï¼‰"""
        self.logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
        
        car_data_list = []
        current_url = url
        page_count = 1
        car_name = None
        
        while current_url and page_count <= max_pages:
            try:
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_count} ã‚’å‡¦ç†ä¸­...")
                
                response = self.session.get(current_url, timeout=10)
                response.raise_for_status()
                response.encoding = response.apparent_encoding
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # 1ãƒšãƒ¼ã‚¸ç›®ã§è»Šç¨®åã‚’å–å¾—
                if page_count == 1:
                    car_name = self.extract_car_name(soup, url)
                    self.logger.info(f"è»Šç¨®å: {car_name}")
                
                # è»Šä¸¡ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
                car_items = soup.find_all('div', class_='cassette js_listTableCassette')
                if not car_items:
                    self.logger.warning("è»Šä¸¡æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    break
                
                # å‡¦ç†ä»¶æ•°åˆ¶é™
                items_to_process = car_items[:max_items_per_page]
                self.logger.info(f"{len(car_items)}å°ä¸­ {len(items_to_process)}å°ã‚’å‡¦ç†")
                
                for item in items_to_process:
                    car_data = self.parse_car_item(item, car_name, current_url)
                    if car_data:
                        car_data_list.append(car_data)
                
                # æ¬¡ãƒšãƒ¼ã‚¸URLå–å¾—
                next_button = soup.select_one('button.pager__btn__next:not([disabled])')
                if next_button and next_button.has_attr('onclick'):
                    match = re.search(r"location\.href='([^']*)'", next_button['onclick'])
                    if match:
                        current_url = urljoin(current_url, match.group(1))
                        page_count += 1
                        time.sleep(1)
                    else:
                        break
                else:
                    break
                    
            except requests.RequestException as e:
                self.logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                break
            except Exception as e:
                self.logger.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
                break
        
        self.logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(car_data_list)}å°")
        return car_data_list, car_name
    
    def save_data(self, car_data_list, car_name):
        """ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆå–å¾—æ—¥æ™‚ã¨URLæƒ…å ±ä»˜ãï¼‰"""
        if not car_data_list:
            self.logger.warning("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        
        # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        today = datetime.now()
        car_folder = self.output_dir / self.sanitize_filename(car_name)
        date_folder = car_folder / today.strftime('%Yå¹´%mæœˆ%dæ—¥')
        date_folder.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·æ±ºå®š
        base_filename = f"{today.strftime('%Y_%m_%d')}_{self.sanitize_filename(car_name)}"
        file_number = 1
        while True:
            csv_path = date_folder / f"{base_filename}.No{file_number}.csv"
            if not csv_path.exists():
                break
            file_number += 1
        
        final_filename = f"{base_filename}.No{file_number}"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆãƒ»ä¿å­˜
        df = pd.DataFrame(car_data_list)
        
        # åˆ—ã®é †åºã‚’æŒ‡å®šï¼ˆæ–°ã—ã„åˆ—ã‚’å«ã‚€ï¼‰
        column_order = [
            'è»Šç¨®å', 'ãƒ¢ãƒ‡ãƒ«', 'ã‚°ãƒ¬ãƒ¼ãƒ‰', 'æ”¯æ‰•ç·é¡', 'å¹´å¼', 'èµ°è¡Œè·é›¢', 
            'ä¿®å¾©æ­´', 'ãƒŸãƒƒã‚·ãƒ§ãƒ³', 'æ’æ°—é‡', 'å–å¾—æ—¥æ™‚', 'å–å¾—æ—¥', 'å–å¾—æ™‚åˆ»',
            'ã‚½ãƒ¼ã‚¹URL', 'è»Šä¸¡URL'
        ]
        
        # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é¸æŠ
        available_columns = [col for col in column_order if col in df.columns]
        df_ordered = df[available_columns]
        
        # CSVä¿å­˜
        csv_path = date_folder / f"{final_filename}.csv"
        df_ordered.to_csv(csv_path, index=False, encoding='utf-8-sig')
        self.logger.info(f"CSVä¿å­˜: {csv_path}")
        
        # Excelä¿å­˜
        try:
            excel_path = date_folder / f"{final_filename}.xlsx"
            
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
                df_ordered.to_excel(writer, sheet_name='è»Šä¸¡ãƒ‡ãƒ¼ã‚¿', index=False)
                
                # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æƒ…å ±
                scraping_info = pd.DataFrame([{
                    'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹æ™‚åˆ»': self.scraping_start_time.isoformat(),
                    'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†æ™‚åˆ»': datetime.now().isoformat(),
                    'å–å¾—å°æ•°': len(car_data_list),
                    'è»Šç¨®å': car_name,
                    'ãƒ•ã‚¡ã‚¤ãƒ«å': final_filename
                }])
                scraping_info.to_excel(writer, sheet_name='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æƒ…å ±', index=False)
                
            self.logger.info(f"Excelä¿å­˜: {excel_path}")
        except ImportError:
            self.logger.warning("openpyxlãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return csv_path
    
    def run_from_urls_file(self, urls_file=None):
        """URLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œï¼ˆãƒ‘ã‚¹è‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½ä»˜ãï¼‰"""
        # URLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è‡ªå‹•æ¤œå‡º
        if urls_file is None:
            possible_paths = [
                "urls.txt",                           # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                "../urls.txt",                        # ä¸€ã¤ä¸Šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                "../../urls.txt",                     # äºŒã¤ä¸Šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                str(Path(__file__).parent.parent / "urls.txt"),  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
                str(Path(__file__).parent.parent.parent / "urls.txt")  # ã•ã‚‰ã«ä¸Š
            ]
            
            urls_file = None
            for path in possible_paths:
                if os.path.exists(path):
                    urls_file = path
                    self.logger.info(f"URLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {path}")
                    break
            
            if urls_file is None:
                self.logger.error("URLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®å ´æ‰€ã«urls.txtã‚’é…ç½®ã—ã¦ãã ã•ã„:")
                for path in possible_paths:
                    self.logger.error(f"  - {os.path.abspath(path)}")
                return []
        
        if not os.path.exists(urls_file):
            self.logger.error(f"URLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {urls_file}")
            return []
        
        with open(urls_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f 
                   if line.strip() and not line.startswith('#')]
        
        if not urls:
            self.logger.error("æœ‰åŠ¹ãªURLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            # ã‚µãƒ³ãƒ—ãƒ«URLã‚’ä½œæˆ
            self.create_sample_urls_file(urls_file)
            return []
        
        self.logger.info(f"{len(urls)}ä»¶ã®URLã‚’å‡¦ç†ã—ã¾ã™")
        
        results = []
        for i, url in enumerate(urls):
            try:
                self.logger.info(f"URL {i+1}/{len(urls)} ã‚’å‡¦ç†ä¸­: {url}")
                car_data_list, car_name = self.scrape_url(url)
                if car_data_list:
                    saved_path = self.save_data(car_data_list, car_name)
                    if saved_path:
                        results.append(saved_path)
                        
                        # é€²æ—ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                        self.logger.info(f"å®Œäº†: {car_name} - {len(car_data_list)}å°å–å¾—")
                
                # æ¬¡URLã¾ã§å¾…æ©Ÿ
                if i < len(urls) - 1:
                    self.logger.info("æ¬¡ã®URLå‡¦ç†ã¾ã§5ç§’å¾…æ©Ÿ...")
                    time.sleep(5)
                    
            except Exception as e:
                self.logger.error(f"URLå‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                continue
        
        # å®Œäº†ã‚µãƒãƒªãƒ¼
        total_files = len(results)
        self.logger.info("=" * 50)
        self.logger.info("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ã‚µãƒãƒªãƒ¼")
        self.logger.info("=" * 50)
        self.logger.info(f"å‡¦ç†URLæ•°: {len(urls)}")
        self.logger.info(f"ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
        self.logger.info(f"é–‹å§‹æ™‚åˆ»: {self.scraping_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return results
    
    def create_sample_urls_file(self, urls_file):
        """ã‚µãƒ³ãƒ—ãƒ«URLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        sample_urls = [
            "# RC Fç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°URLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«",
            "# ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯#ã§å§‹ã‚ã¦ãã ã•ã„",
            "",
            "# RC F å…¨å›½æ¤œç´¢",
            "https://www.carsensor.net/usedcar/search.php?STID=CS210610&CARC=LE_S016",
            "",
            "# RC F æ±äº¬éƒ½å†…",
            "https://www.carsensor.net/usedcar/search.php?STID=CS210610&CARC=LE_S016&AREA=1",
            "",
            "# RC F å¤§é˜ªåºœå†…", 
            "https://www.carsensor.net/usedcar/search.php?STID=CS210610&CARC=LE_S016&AREA=2",
            "",
            "# ä½¿ç”¨æ–¹æ³•:",
            "# 1. ä¸Šè¨˜URLã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆ#ï¼‰ã‚’å¤–ã™",
            "# 2. å¿…è¦ã«å¿œã˜ã¦URLã‚’è¿½åŠ ãƒ»ç·¨é›†",
            "# 3. python car_scraper.py ã§å®Ÿè¡Œ"
        ]
        
        try:
            with open(urls_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sample_urls))
            self.logger.info(f"ã‚µãƒ³ãƒ—ãƒ«URLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {urls_file}")
            self.logger.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦URLã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
        except Exception as e:
            self.logger.error(f"ã‚µãƒ³ãƒ—ãƒ«URLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = CarScraper()
    results = scraper.run_from_urls_file()
    
    if results:
        print("\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
        print("ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        for path in results:
            print(f"  - {path}")
        
        print("\nğŸ“Š æ–°æ©Ÿèƒ½:")
        print("  â€¢ å–å¾—æ—¥æ™‚ã®è¨˜éŒ²ï¼ˆISOå½¢å¼ï¼‰")
        print("  â€¢ è»Šä¸¡å€‹åˆ¥URLã®æŠ½å‡º")
        print("  â€¢ å–å¾—æ—¥ãƒ»å–å¾—æ™‚åˆ»ã®åˆ†é›¢")
        print("  â€¢ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æƒ…å ±ã®è¨˜éŒ²")
        
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == '__main__':
    main()