#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±åˆã‚«ãƒ¼ã‚»ãƒ³ã‚µãƒ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
Windowsç’°å¢ƒå¯¾å¿œãƒ»ã‚¨ãƒ©ãƒ¼å‡¦ç†å¼·åŒ–ç‰ˆ
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
import re
import logging
from datetime import datetime
from urllib.parse import urljoin
from pathlib import Path

class CarScraper:
    def __init__(self, output_dir="data\\scraped"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.setup_logging()
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        })
        
    def setup_logging(self):
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def sanitize_filename(self, name):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ãˆãªã„æ–‡å­—ã‚’ç½®æ›ãƒ»å‰Šé™¤"""
        name = name.replace('ãƒ»', '_').replace(' ', '_')
        return re.sub(r'[\\|/|:|*|?|"|<|>|\|]', '', name)
    
    def extract_car_name(self, soup, url):
        """è»Šç¨®åã‚’æŠ½å‡º"""
        selectors = [
            ('h2.title1', r'(.+?)ï¼ˆå…¨å›½ï¼‰ã®ä¸­å¤è»Š'),
            ('h1', r'(.+?)\s*ã®ä¸­å¤è»Š'),
            ('title', r'(.+?)\s*ã®ä¸­å¤è»Š')
        ]
        
        for selector, pattern in selectors:
            element = soup.select_one(selector)
            if element:
                match = re.search(pattern, element.get_text())
                if match:
                    full_name = match.group(1).strip()
                    return full_name.split()[-1].split('ãƒ»')[-1]
        
        self.logger.warning(f"è»Šç¨®åã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ: {url}")
        return "Unknown"
    
    def parse_car_item(self, item, car_name):
        """å€‹åˆ¥è»Šä¸¡ã‚¢ã‚¤ãƒ†ãƒ ã®è§£æ"""
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚°ãƒ¬ãƒ¼ãƒ‰
            title_tag = item.find('h3', class_='cassetteMain__title')
            full_title = title_tag.get_text(strip=True) if title_tag else 'æƒ…å ±ãªã—'
            grade = full_title.replace(car_name, '', 1).strip()
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
            model_tag = item.find('p', class_='cassetteMain__tag')
            model_info = model_tag.get_text(strip=True) if model_tag else 'æƒ…å ±ãªã—'
            
            # ä¾¡æ ¼
            price_tag = item.find('div', class_='totalPrice')
            if price_tag and price_tag.find('p', class_='totalPrice__content'):
                price = price_tag.find('p', class_='totalPrice__content').get_text(strip=True)
            else:
                price = 'å¿œè«‡'
            
            # ä»•æ§˜æƒ…å ±
            spec_data = {
                'å¹´å¼': 'æƒ…å ±ãªã—',
                'èµ°è¡Œè·é›¢': 'æƒ…å ±ãªã—', 
                'ä¿®å¾©æ­´': 'æƒ…å ±ãªã—',
                'ãƒŸãƒƒã‚·ãƒ§ãƒ³': 'æƒ…å ±ãªã—',
                'æ’æ°—é‡': 'æƒ…å ±ãªã—'
            }
            
            spec_items = item.select('dl.specList > div.specList__detailBox')
            for spec_item in spec_items:
                dt = spec_item.find('dt')
                dd = spec_item.find('dd')
                if dt and dd:
                    label = dt.get_text(strip=True)
                    value = dd.get_text(strip=True)
                    if label in spec_data:
                        spec_data[label] = value
            
            return {
                'è»Šç¨®å': car_name,
                'ãƒ¢ãƒ‡ãƒ«': model_info,
                'ã‚°ãƒ¬ãƒ¼ãƒ‰': grade,
                'æ”¯æ‰•ç·é¡': price,
                'å¹´å¼': spec_data['å¹´å¼'],
                'èµ°è¡Œè·é›¢': spec_data['èµ°è¡Œè·é›¢'],
                'ä¿®å¾©æ­´': spec_data['ä¿®å¾©æ­´'],
                'ãƒŸãƒƒã‚·ãƒ§ãƒ³': spec_data['ãƒŸãƒƒã‚·ãƒ§ãƒ³'],
                'æ’æ°—é‡': spec_data['æ’æ°—é‡'],
                'å–å¾—æ—¥æ™‚': datetime.now().isoformat(),
                'ã‚½ãƒ¼ã‚¹URL': ''  # å¾Œã§è¨­å®š
            }
            
        except Exception as e:
            self.logger.warning(f"è»Šä¸¡ã‚¢ã‚¤ãƒ†ãƒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def scrape_url(self, url, max_pages=10, max_items_per_page=30):
        """å˜ä¸€URLã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        self.logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
        
        car_data_list = []
        current_url = url
        page_count = 1
        car_name = None
        
        while current_url and page_count <= max_pages:
            try:
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_count} ã‚’å‡¦ç†ä¸­...")
                
                response = self.session.get(current_url, timeout=10)
                response.raise_for_status()
                response.encoding = response.apparent_encoding
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # 1ãƒšãƒ¼ã‚¸ç›®ã§è»Šç¨®åã‚’å–å¾—
                if page_count == 1:
                    car_name = self.extract_car_name(soup, url)
                    self.logger.info(f"è»Šç¨®å: {car_name}")
                
                # è»Šä¸¡ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
                car_items = soup.find_all('div', class_='cassette js_listTableCassette')
                if not car_items:
                    self.logger.warning("è»Šä¸¡æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    break
                
                # å‡¦ç†ä»¶æ•°åˆ¶é™
                items_to_process = car_items[:max_items_per_page]
                self.logger.info(f"{len(car_items)}å°ä¸­ {len(items_to_process)}å°ã‚’å‡¦ç†")
                
                for item in items_to_process:
                    car_data = self.parse_car_item(item, car_name)
                    if car_data:
                        car_data['ã‚½ãƒ¼ã‚¹URL'] = current_url
                        car_data_list.append(car_data)
                
                # æ¬¡ãƒšãƒ¼ã‚¸URLå–å¾—
                next_button = soup.select_one('button.pager__btn__next:not([disabled])')
                if next_button and next_button.has_attr('onclick'):
                    match = re.search(r"location\.href='([^']*)'", next_button['onclick'])
                    if match:
                        current_url = urljoin(current_url, match.group(1))
                        page_count += 1
                        time.sleep(1)
                    else:
                        break
                else:
                    break
                    
            except requests.RequestException as e:
                self.logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                break
            except Exception as e:
                self.logger.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
                break
        
        self.logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(car_data_list)}å°")
        return car_data_list, car_name
    
    def save_data(self, car_data_list, car_name):
        """ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        if not car_data_list:
            self.logger.warning("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
        
        # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        today = datetime.now()
        car_folder = self.output_dir / self.sanitize_filename(car_name)
        date_folder = car_folder / today.strftime('%Yå¹´%mæœˆ%dæ—¥')
        date_folder.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·æ±ºå®š
        base_filename = f"{today.strftime('%Y_%m_%d')}_{self.sanitize_filename(car_name)}"
        file_number = 1
        while True:
            csv_path = date_folder / f"{base_filename}.No{file_number}.csv"
            if not csv_path.exists():
                break
            file_number += 1
        
        final_filename = f"{base_filename}.No{file_number}"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆãƒ»ä¿å­˜
        df = pd.DataFrame(car_data_list)
        
        # CSVä¿å­˜
        csv_path = date_folder / f"{final_filename}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        self.logger.info(f"CSVä¿å­˜: {csv_path}")
        
        # Excelä¿å­˜
        try:
            excel_path = date_folder / f"{final_filename}.xlsx"
            df.to_excel(excel_path, index=False, engine='openpyxl')
            self.logger.info(f"Excelä¿å­˜: {excel_path}")
        except ImportError:
            self.logger.warning("openpyxlãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return csv_path
    
    def run_from_urls_file(self, urls_file="urls.txt"):
        """URLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        if not os.path.exists(urls_file):
            self.logger.error(f"URLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {urls_file}")
            return []
        
        with open(urls_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f 
                   if line.strip() and not line.startswith('#')]
        
        if not urls:
            self.logger.error("æœ‰åŠ¹ãªURLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return []
        
        self.logger.info(f"{len(urls)}ä»¶ã®URLã‚’å‡¦ç†ã—ã¾ã™")
        
        results = []
        for i, url in enumerate(urls):
            try:
                car_data_list, car_name = self.scrape_url(url)
                if car_data_list:
                    saved_path = self.save_data(car_data_list, car_name)
                    if saved_path:
                        results.append(saved_path)
                
                # æ¬¡URLã¾ã§å¾…æ©Ÿ
                if i < len(urls) - 1:
                    self.logger.info("æ¬¡ã®URLå‡¦ç†ã¾ã§5ç§’å¾…æ©Ÿ...")
                    time.sleep(5)
                    
            except Exception as e:
                self.logger.error(f"URLå‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                continue
        
        self.logger.info(f"å…¨å‡¦ç†å®Œäº†: {len(results)}ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ")
        return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = CarScraper()
    results = scraper.run_from_urls_file()
    
    if results:
        print("\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")
        for path in results:
            print(f"  ğŸ“„ {path}")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == '__main__':
    main()