#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±åˆä¸­å¤è»Šåˆ†æã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‹ã‚‰åˆ†æã¾ã§ä¸€æ‹¬å‡¦ç†
"""

import sys
import os
import re
import argparse
import pandas as pd
import logging
from pathlib import Path
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.scraper.car_scraper import CarScraper
from src.analyzer.grade_normalizer import GradeNormalizer
from src.utils import get_car_directories

class CarAnalysisSystem:
    def __init__(self):
        self.project_root = project_root
        self.setup_logging()

    def available_car_dirs(self):
        """Return available car directory tuples ``(name, path)``."""
        return get_car_directories(self.project_root)
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_dir = self.project_root / 'logs'
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'system.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def scrape_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        self.logger.info("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        scraper = CarScraper()
        results = scraper.run_from_urls_file(str(self.project_root / 'urls.txt'))
        
        if results:
            self.logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(results)}ãƒ•ã‚¡ã‚¤ãƒ«")
            return results
        else:
            self.logger.warning("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return []
    
    def analyze_data(self, data_path=None, car_name=None, car_dir=None, use_latest=False):
        """ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        # åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å®š
        if data_path:
            target_file = Path(data_path)
        elif car_dir:
            target_file = self.find_car_data_in_dir(Path(car_dir), use_latest)
        elif car_name:
            target_file = self.find_car_data_file(car_name, use_latest)
        else:
            target_file = self.select_data_file_interactive()
        
        if not target_file or not target_file.exists():
            self.logger.error(f"åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_file}")
            return None
        
        self.logger.info(f"åˆ†æé–‹å§‹: {target_file}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            if target_file.suffix.lower() == '.csv':
                df = pd.read_csv(target_file, encoding='utf-8-sig')
            elif target_file.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(target_file)
            else:
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {target_file.suffix}")
            
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
            
            # ã‚°ãƒ¬ãƒ¼ãƒ‰æ­£è¦åŒ–
            normalizer = GradeNormalizer()
            normalized_df = normalizer.normalize_dataframe(df)
            
            # åˆ†æçµæœä¿å­˜
            output_path = self.save_analysis_result(normalized_df, target_file)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = normalizer.get_normalization_report(normalized_df)
            self.print_analysis_report(report, target_file)
            
            self.logger.info(f"åˆ†æå®Œäº†: {output_path}")
            return output_path
            
        except Exception as e:
            self.logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def find_car_data_file(self, car_name, use_latest=True):
        """è»Šç¨®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        from src.utils import get_scraped_dir
        scraped_dir = get_scraped_dir(self.project_root)
        car_dirs = list(scraped_dir.glob(f"*{car_name}*"))
        
        if not car_dirs:
            self.logger.error(f"è»Šç¨®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {car_name}")
            return None
        
        # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
        all_files = []
        for car_dir in car_dirs:
            for date_dir in car_dir.iterdir():
                if date_dir.is_dir():
                    csv_files = list(date_dir.glob('*.csv'))
                    all_files.extend(csv_files)
        
        if not all_files:
            return None
        
        if use_latest:
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
            return max(all_files, key=lambda f: f.stat().st_mtime)
        else:
            # é¸æŠè‚¢ã‚’è¡¨ç¤º
            return self.select_from_files(all_files)

    def find_car_data_in_dir(self, car_dir: Path, use_latest=True):
        """Search a directory for car data files."""
        if not car_dir.exists():
            self.logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {car_dir}")
            return None

        all_files = []
        for date_dir in car_dir.iterdir():
            if date_dir.is_dir():
                csv_files = list(date_dir.glob('*.csv'))
                all_files.extend(csv_files)

        if not all_files:
            self.logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {car_dir}")
            return None

        if use_latest:
            return max(all_files, key=lambda f: f.stat().st_mtime)
        else:
            return self.select_from_files(all_files)
    
    def select_from_files(self, files):
        """ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ"""
        if not files:
            return None
        
        print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«:")
        for i, file_path in enumerate(files, 1):
            print(f"{i}. {file_path.name} ({file_path.parent.name})")
        
        try:
            choice = int(input("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ (ç•ªå·): "))
            if 1 <= choice <= len(files):
                return files[choice - 1]
        except ValueError:
            pass
        
        print("ç„¡åŠ¹ãªé¸æŠã§ã™")
        return None
    
    def select_data_file_interactive(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ"""
        from src.utils import get_scraped_dir
        scraped_dir = get_scraped_dir(self.project_root)
        
        if not scraped_dir.exists():
            print("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # è»Šç¨®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§
        car_dirs = [d for d in scraped_dir.iterdir() if d.is_dir()]
        
        if not car_dirs:
            print("è»Šç¨®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        print("\nåˆ©ç”¨å¯èƒ½ãªè»Šç¨®:")
        for i, car_dir in enumerate(car_dirs, 1):
            print(f"{i}. {car_dir.name}")
        
        try:
            choice = int(input("è»Šç¨®ã‚’é¸æŠ (ç•ªå·): "))
            if 1 <= choice <= len(car_dirs):
                selected_car = car_dirs[choice - 1]
                
                # è©²å½“è»Šç¨®ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
                all_files = []
                for date_dir in selected_car.iterdir():
                    if date_dir.is_dir():
                        csv_files = list(date_dir.glob('*.csv'))
                        all_files.extend(csv_files)
                
                return self.select_from_files(all_files)
                
        except ValueError:
            pass
        
        print("ç„¡åŠ¹ãªé¸æŠã§ã™")
        return None
    
    def save_analysis_result(self, df, source_file):
        """åˆ†æçµæœä¿å­˜"""
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_dir = self.project_root / 'data' / 'normalized'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        car_name = df['è»Šç¨®å'].iloc[0] if 'è»Šç¨®å' in df.columns else 'Unknown'
        output_filename = f"{car_name}_normalized_{timestamp}.xlsx"
        output_path = output_dir / output_filename
        
        # Excelä¿å­˜ï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆï¼‰
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # æ­£è¦åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            df.to_excel(writer, sheet_name='æ­£è¦åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿', index=False)
            
            # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥é›†è¨ˆ
            if 'æ­£è¦ã‚°ãƒ¬ãƒ¼ãƒ‰' in df.columns:
                grade_summary = df.groupby('æ­£è¦ã‚°ãƒ¬ãƒ¼ãƒ‰').agg({
                    'æ”¯æ‰•ç·é¡': lambda x: self.extract_price_stats(x),
                    'ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦': 'mean'
                }).round(2)
                grade_summary.to_excel(writer, sheet_name='ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥é›†è¨ˆ')
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata = {
                'ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«': str(source_file),
                'å‡¦ç†æ—¥æ™‚': datetime.now().isoformat(),
                'ç·ä»¶æ•°': len(df),
                'æ­£è¦ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°': df['æ­£è¦ã‚°ãƒ¬ãƒ¼ãƒ‰'].nunique() if 'æ­£è¦ã‚°ãƒ¬ãƒ¼ãƒ‰' in df.columns else 0
            }
            metadata_df = pd.DataFrame([metadata])
            metadata_df.to_excel(writer, sheet_name='ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿', index=False)
        
        return output_path
    
    def extract_price_stats(self, price_series):
        """ä¾¡æ ¼çµ±è¨ˆè¨ˆç®—"""
        values = []
        for price in price_series:
            if pd.isna(price):
                continue
            match = re.search(r'([0-9.]+)ä¸‡å††', str(price))
            if match:
                values.append(float(match.group(1)))
        
        if values:
            import numpy as np
            return {
                'å¹³å‡': np.mean(values),
                'æœ€å°': np.min(values),
                'æœ€å¤§': np.max(values),
                'ä»¶æ•°': len(values)
            }
        return {'å¹³å‡': 0, 'æœ€å°': 0, 'æœ€å¤§': 0, 'ä»¶æ•°': 0}
    
    def print_analysis_report(self, report, source_file):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)
        print(f"ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {source_file.name}")
        print(f"å‡¦ç†æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        if report:
            print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
            print(f"  ç·ãƒ‡ãƒ¼ã‚¿æ•°: {report['total_count']:,}ä»¶")
            print(f"  å…ƒã‚°ãƒ¬ãƒ¼ãƒ‰æ•°: {report['unique_original_grades']}ç¨®é¡")
            print(f"  æ­£è¦ã‚°ãƒ¬ãƒ¼ãƒ‰æ•°: {report['unique_normalized_grades']}ç¨®é¡")
            
            quality = report.get('matching_quality', {})
            print(f"\nğŸ¯ ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦:")
            print(f"  é«˜ç²¾åº¦(â‰¥80%): {quality.get('high_confidence', 0)}ä»¶")
            print(f"  ä¸­ç²¾åº¦(60-80%): {quality.get('medium_confidence', 0)}ä»¶")
            print(f"  ä½ç²¾åº¦(<60%): {quality.get('low_confidence', 0)}ä»¶")
            
            print(f"\nğŸ“‹ ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°(ä¸Šä½5ä»¶):")
            grade_dist = report.get('grade_distribution', {})
            for grade, count in list(grade_dist.items())[:5]:
                print(f"  {grade}: {count}ä»¶")
    
    def list_available_data(self):
        """åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ä¸€è¦§è¡¨ç¤º"""
        from src.utils import get_scraped_dir
        scraped_dir = get_scraped_dir(self.project_root)
        
        if not scraped_dir.exists():
            print("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        print("ğŸ“ åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿:")
        print("-" * 50)
        
        for car_dir in scraped_dir.iterdir():
            if car_dir.is_dir():
                print(f"\nğŸš— {car_dir.name}")
                
                for date_dir in car_dir.iterdir():
                    if date_dir.is_dir():
                        csv_files = list(date_dir.glob('*.csv'))
                        for csv_file in csv_files:
                            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå–å¾—
                            size_mb = csv_file.stat().st_size / (1024 * 1024)
                            print(f"  ğŸ“„ {csv_file.name} ({size_mb:.1f}MB)")
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("ğŸš— çµ±åˆä¸­å¤è»Šåˆ†æã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 50)
        
        while True:
            print("\né¸æŠã—ã¦ãã ã•ã„:")
            print("1. ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
            print("2. ãƒ‡ãƒ¼ã‚¿åˆ†æ")
            print("3. åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ä¸€è¦§")
            print("4. å…¨å·¥ç¨‹å®Ÿè¡Œ")
            print("5. çµ‚äº†")
            
            choice = input("\né¸æŠ (1-5): ").strip()
            
            if choice == '1':
                self.scrape_data()
            elif choice == '2':
                self.analyze_data()
            elif choice == '3':
                self.list_available_data()
            elif choice == '4':
                scraped_files = self.scrape_data()
                for file_path in scraped_files:
                    self.analyze_data(data_path=file_path)
            elif choice == '5':
                print("ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                break
            else:
                print("ç„¡åŠ¹ãªé¸æŠã§ã™")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    car_dirs = get_car_directories(project_root)
    epilog = ""
    if car_dirs:
        lines = ["åˆ©ç”¨å¯èƒ½ãªè»Šç¨®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:"]
        for name, path in car_dirs:
            lines.append(f"  {name}: {path}")
        epilog = "\n".join(lines)

    parser = argparse.ArgumentParser(description='çµ±åˆä¸­å¤è»Šåˆ†æã‚·ã‚¹ãƒ†ãƒ ', epilog=epilog)
    parser.add_argument('--scrape', action='store_true', help='ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ')
    parser.add_argument('--analyze', action='store_true', help='åˆ†æå®Ÿè¡Œ')
    parser.add_argument('--all', action='store_true', help='å…¨å·¥ç¨‹å®Ÿè¡Œ')
    parser.add_argument('--interactive', action='store_true', help='ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰')
    
    parser.add_argument('--path', help='åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--car', help='è»Šç¨®å')
    parser.add_argument('--dir', help='è»Šç¨®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹')
    parser.add_argument('--latest', action='store_true', help='æœ€æ–°ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨')
    parser.add_argument('--list', action='store_true', help='åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ä¸€è¦§')
    
    args = parser.parse_args()
    
    system = CarAnalysisSystem()
    
    try:
        if args.interactive:
            system.interactive_mode()
        elif args.all:
            print("ğŸš€ å…¨å·¥ç¨‹ã‚’å®Ÿè¡Œã—ã¾ã™")
            scraped_files = system.scrape_data()
            for file_path in scraped_files:
                system.analyze_data(data_path=file_path)
        elif args.scrape:
            system.scrape_data()
        elif args.analyze:
            system.analyze_data(
                data_path=args.path,
                car_name=args.car,
                car_dir=args.dir,
                use_latest=args.latest
            )
        elif args.list:
            system.list_available_data()
        else:
            parser.print_help()
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()
